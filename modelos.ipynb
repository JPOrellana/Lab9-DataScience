{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science - Laboratorio 9\n",
    "## Visualizaciones interactivas y dashboards\n",
    "### Modelos LSTM\n",
    "---\n",
    "**Integrantes**\n",
    "- Diego Alberto Leiva\n",
    "- José Pablo Orellana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulacion de Datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sistema\n",
    "import os\n",
    "\n",
    "# Utilidades\n",
    "import random\n",
    "\n",
    "# Preprocesamiento\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# PyTorch CUDA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer los archivos de una carpeta y almacenarlos como dataframes\n",
    "consumos = pd.read_csv('data/consumos_clean.csv', sep=';', encoding='utf-8')\n",
    "importaciones = pd.read_csv('data/importaciones_clean.csv', sep=';', encoding='utf-8')\n",
    "precios = pd.read_csv('data/precios_clean.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuracion de PyTorch con CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifica si el sistema tiene capacidad de CUDA o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA AVAILABLE\n",
      "Using: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Configurar la semilla para la reproducibilidad\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA AVAILABLE\")\n",
    "\n",
    "    print(f\"Using: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    # Configurar el generador de números aleatorios de CUDA\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # Multi-GPU.\n",
    "\n",
    "    # Configurar el dispositivo de PyTorch para usar la GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # Configurar PyTorch a determinista\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "else:\n",
    "    # Si no hay GPU disponible, usar la CPU\n",
    "    print(\"CUDA NOT AVAILABLE\")\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura de una LSTM con PyTorch CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model():\n",
    "    \"\"\"\n",
    "    Clase para entrenar un modelo LSTM para predecir series temporales.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Número de features en la entrada.\n",
    "        output_size (int): Número de features en la salida.\n",
    "        hidden_layers (int): Número de capas ocultas.\n",
    "        num_layers (int): Número de capas LSTM.\n",
    "        lookback (int): Número de pasos de tiempo hacia atrás.\n",
    "        n_epochs (int): Número de épocas.\n",
    "        test_size (int): Tamaño del conjunto de prueba.\n",
    "        device (torch.device): Dispositivo de PyTorch.\n",
    "        dataframe (pandas.DataFrame): Dataframe de entrada.\n",
    "        target_column (str): Columna objetivo.\n",
    "\n",
    "    Attributes:\n",
    "        input_size (int): Número de features en la entrada.\n",
    "        output_size (int): Número de features en la salida.\n",
    "        hidden_layers (int): Número de capas ocultas.\n",
    "        num_layers (int): Número de capas LSTM.\n",
    "        lookback (int): Número de pasos de tiempo hacia atrás.\n",
    "        n_epochs (int): Número de épocas.\n",
    "        test_size (int): Tamaño del conjunto de prueba.\n",
    "        device (torch.device): Dispositivo de PyTorch.\n",
    "        dataframe (pandas.DataFrame): Dataframe de entrada.\n",
    "        target_column (str): Columna objetivo.\n",
    "        model (torch.nn.Module): Modelo LSTM.\n",
    "        loss_fn (torch.nn.Module): Función de pérdida.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador.\n",
    "        loss_train (list): Lista para almacenar la pérdida de entrenamiento.\n",
    "        loss_test (list): Lista para almacenar la pérdida de prueba.\n",
    "        train_size (int): Tamaño del conjunto de entrenamiento.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_layers, num_layers, lookback, n_epochs, \n",
    "                 test_size, device, dataframe, target_column, train_log, train_res):\n",
    "        \n",
    "        # Inicializar los hiperparámetros\n",
    "        self.input_size = input_size            # Número de features en la entrada\n",
    "        self.output_size = output_size          # Número de features en la salida\n",
    "        self.hidden_layers = hidden_layers      # Número de capas ocultas\n",
    "        self.num_layers = num_layers            # Número de capas LSTM\n",
    "        self.lookback = lookback                # Número de pasos de tiempo hacia atrás\n",
    "        self.n_epochs = n_epochs                # Número de épocas\n",
    "        self.test_size = test_size              # Tamaño del conjunto de prueba\n",
    "        self.device = device                    # Dispositivo de PyTorch\n",
    "        self.dataframe = dataframe              # Dataframe de entrada\n",
    "        self.target_column = target_column      # Columna objetivo\n",
    "        self.train_log = train_log              # Booleano para guardar el log del entrenamiento\n",
    "        self.train_res = train_res              # Booleano para guardar los resultados del entrenamiento\n",
    "\n",
    "        # Crear el modelo LSTM \n",
    "        self.model = self.build_lstm_model()\n",
    "\n",
    "        # Definir la función de pérdida y el optimizador\n",
    "        self.loss_fn = nn.MSELoss().to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        # Listas para almacenar la pérdida\n",
    "        self.loss_train = []\n",
    "        self.loss_test = []\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        self.train_model(dataframe, target_column)\n",
    "\n",
    "\n",
    "    def build_lstm_model(self):\n",
    "        \"\"\"\n",
    "        Función para construir el modelo LSTM.\n",
    "\n",
    "        Returns:\n",
    "            model (torch.nn.Module): El modelo LSTM.\n",
    "        \"\"\"\n",
    "        class LSTM(nn.Module):\n",
    "            def __init__(self, input_size, hidden_layers, output_size, num_layers):\n",
    "                super(LSTM, self).__init__()\n",
    "                \n",
    "                # Definir la capa LSTM\n",
    "                self.lstm = nn.LSTM(input_size = input_size, \n",
    "                                    hidden_size = hidden_layers, \n",
    "                                    num_layers = num_layers, \n",
    "                                    batch_first = True)\n",
    "                \n",
    "                # Definir la capa de salida\n",
    "                self.linear = nn.Linear(hidden_layers, \n",
    "                                        output_size)\n",
    "                \n",
    "                # Agregar una capa de activación ReLU\n",
    "                self.relu = nn.ReLU() \n",
    "                \n",
    "                self.hidden_size = hidden_layers  # Numero de hidden states\n",
    "                self.num_layers = num_layers    # Numero de capas\n",
    "                \n",
    "\n",
    "            def forward(self, x):\n",
    "                # Inicializar los hidden states\n",
    "                h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "                c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "\n",
    "                # Forward pass a la capa LSTM\n",
    "                out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "                # Aplicar la capa de activación\n",
    "                out = self.relu(out)\n",
    "                \n",
    "                # Solo se necesita el último hidden state\n",
    "                out = self.linear(out)\n",
    "                \n",
    "                return out\n",
    "            \n",
    "        # Retornar el modelo LSTM con los hiperparámetros dados\n",
    "        return LSTM(self.input_size, self.hidden_layers, self.output_size, self.num_layers).to(self.device)\n",
    "    \n",
    "\n",
    "\n",
    "    def create_dataset(self, dataset):\n",
    "        \"\"\"\n",
    "        Función para crear el dataset de entrenamiento.\n",
    "\n",
    "        Args:\n",
    "            dataset (numpy.array): El dataset de entrada.\n",
    "\n",
    "        Returns:\n",
    "            X (torch.Tensor): El tensor de features.\n",
    "            y (torch.Tensor): El tensor de target.\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(len(dataset) - self.lookback):\n",
    "            feature = dataset[i : i + self.lookback]\n",
    "            target = dataset[i + 1 : i + self.lookback + 1]\n",
    "            X.append(feature)\n",
    "            y.append(target)\n",
    "\n",
    "        # Convertir listas a numpy arrays y luego a tensores\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "\n",
    "    def train_model(self, dataframe, variable, ):\n",
    "        \"\"\"\n",
    "        Función para entrenar el modelo LSTM.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): El dataframe de entrada.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Copiar el dataframe\n",
    "        df = dataframe.copy()\n",
    "        # Seleccionar la variable objetivo\n",
    "        timeseries = df[[variable]].values.astype(float)\n",
    "        # Normalizar los datos\n",
    "        scaler = StandardScaler()\n",
    "        timeseries = scaler.fit_transform(timeseries)\n",
    "\n",
    "        # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "        self.train_size = len(timeseries) - self.test_size\n",
    "        train_data = timeseries[:self.train_size]\n",
    "        test_data = timeseries[self.train_size:]\n",
    "\n",
    "        # Crear el dataset de entrenamiento y prueba \n",
    "        X_train, y_train = self.create_dataset(train_data)\n",
    "        X_test, y_test = self.create_dataset(test_data)\n",
    "\n",
    "        # Mover los tensores al dispositivo\n",
    "        X_train, y_train = X_train.to(self.device), y_train.to(self.device)\n",
    "        X_test, y_test = X_test.to(self.device), y_test.to(self.device)\n",
    "\n",
    "        # Crear un DataLoader para cargar los datos\n",
    "        loader = data.DataLoader(data.TensorDataset(X_train, y_train), batch_size=16, shuffle=False)\n",
    "\n",
    "        # Iteramos sobre cada epoca\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Colocamos el modelo en modo de entrenamiento\n",
    "            self.model.train()\n",
    "\n",
    "            # Cargamos los batches\n",
    "            for X_batch, y_batch in loader:\n",
    "                # Movemos los datos al dispositivo\n",
    "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "\n",
    "                # Obtenemos una primera prediccion\n",
    "                y_pred = self.model(X_batch)\n",
    "                # Calculamos la perdida\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                # Reseteamos la gradiente a cero\n",
    "                #   sino la gradiente de previas iteraciones se acumulará con las nuevas\n",
    "                self.optimizer.zero_grad()\n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                # Aplicar las gradientes para actualizar los parametros del modelo\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Validación cada 100 epocas\n",
    "            if epoch % 25 == 0 or epoch == self.n_epochs - 1:\n",
    "                # Colocamos el modelo en modo de evaluación\n",
    "                self.model.eval()\n",
    "\n",
    "                # Deshabilitamos el calculo de gradientes\n",
    "                with torch.no_grad():\n",
    "                    # Prediccion de train\n",
    "                    y_pred = self.model(X_train)\n",
    "                    # Calculo del RMSE - Root Mean Square Error\n",
    "                    train_rmse = np.sqrt(self.loss_fn(y_pred, y_train).cpu().numpy())\n",
    "                    # Prediccion sobre validation\n",
    "                    y_pred = self.model(X_test)\n",
    "                    # Calculo del RMSE para validation\n",
    "                    test_rmse = np.sqrt(self.loss_fn(y_pred, y_test).cpu().numpy())\n",
    "                    # Almacenamos los resultados\n",
    "                    self.loss_train.append(train_rmse)\n",
    "                    self.loss_test.append(test_rmse)\n",
    "                \n",
    "                if self.train_log:\n",
    "                    # Mostramos los resultados\n",
    "                    print(f\"Epoch {epoch}: train RMSE {train_rmse:.4f}, test RMSE {test_rmse:.4f}\")\n",
    "\n",
    "            # Si hay GPU, sincronizamos el dispositivo\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "        # Guardar el las predicciones en un DataFrame\n",
    "        result_df = self.save_results(timeseries, X_train, X_test)\n",
    "\n",
    "        if self.train_res:\n",
    "            # Guardar el DataFrame en un archivo CSV si es necesario\n",
    "            result_df.to_csv(f'models/predictions_{self.target_column}.csv', index=False)\n",
    "            print(f\"Predicciones guardadas en predictions_{self.target_column}.csv\")\n",
    "        else:\n",
    "            print(\"Predicciones no guardadas en CSV, solo retornadas como DataFrame.\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def save_results(self, timeseries, X_train, X_test):\n",
    "        \"\"\"\n",
    "        Función para guardar los resultados en un DataFrame.\n",
    "\n",
    "        Args:\n",
    "            timeseries (numpy.array): La serie temporal original.\n",
    "            X_train (torch.Tensor): El tensor de features de entrenamiento.\n",
    "            X_test (torch.Tensor): El tensor de features de prueba.\n",
    "\n",
    "        Returns:\n",
    "            result_df (pandas.DataFrame): DataFrame con la serie original y las predicciones.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Movemos las predicciones de entrenamiento\n",
    "            train_plot = np.ones_like(timeseries) * np.nan\n",
    "            # Predicciones de entrenamiento\n",
    "            y_train_pred = self.model(X_train)\n",
    "            # Tomamos solo el último paso de tiempo\n",
    "            train_plot[self.lookback:self.train_size] = y_train_pred[:, -1, :].cpu().numpy().flatten()\n",
    "\n",
    "            # Movemos las predicciones de prueba\n",
    "            test_plot = np.ones_like(timeseries) * np.nan\n",
    "            # Predicciones de prueba\n",
    "            y_test_pred = self.model(X_test)\n",
    "            test_plot[self.train_size + self.lookback:len(timeseries)] = y_test_pred[:, -1, :].cpu().numpy().flatten()\n",
    "\n",
    "            # Crear el DataFrame con la serie temporal original y las predicciones\n",
    "            result_df = pd.DataFrame({\n",
    "                'Original Series': timeseries.flatten(),\n",
    "                'Train Predictions': train_plot.flatten(),\n",
    "                'Test Predictions': test_plot.flatten()\n",
    "            })\n",
    "\n",
    "        # Retornamos el DataFrame con los resultados\n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generando Modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSci_L9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
